\chapter{Limitations and Future Work}

There are a lot of limitations on our current work and a lot of potential work in the future. In this chapter, we summarize the limitations in our work from our model to the datasets, and state possible future work to get a better result in real-time image-based camera localization.

\section{Network}

Since our model learns to predict 2D projections of 3D bounding box from limited training data, sometimes it cannot generalize well to objects in new or unusually perspective of views. For example, in our testing data for couch, the result of predicting from back of the couch is not as good as the prediction from the front view.

As we discussed in previous chapter, our loss function does not consider rotation invariant cases. For example, a rectangle table does not have a unique front side. The object looks exactly the same after rotating 180 degrees in xy plane. Thus, the predicted bounding box can be correct assuming either side as the front side. Moreover, if the table is square, the four sides are equal, and each one can be considered as the front side. However, our loss function corresponds each point with its ground truth coordinates, and didn't consider the rotation invariant case. To this end, we should allow the predicted bounding box to rotate in xy plane and set the loss as the minimum loss it ever gets for rotation invariant objects. However, for a round table , we still cannot minimize the loss to zero, as otherwise it will be too much computation overheard.

Our loss function also does not distinguish the error of predicted 2D points and ground truth in small 3D bounding box versus large 3D bounding box. A small error in a large box is generally negligible but the same error in a small box has a much greater effect on the result or IOU. Thus, further improvement on the loss function is possible.

\section{Datasets}
\subsection{SIXD}
\label{sec:SIXD}
Although SIXD tollkit can help us generate quite a lot different views of the object in random textures, the views we generated are still not realistic enough.

To adress these limitations, one of the future work we can do is to adjust the initial orientation of the 3D object model so that most of the camera views have a fixed upright direction aligned to z=1. Also, automatically filtering out some unrealistic views such as the view from the bottom of the object is also necessary. Since in real indoor environment, we rarely see the furniture from a view to the bottom of it. The random background utilized should better be of indoor environments containing recognizable furnitures. And if possible, it would be great if the floor can be detected and put the object onto floor level so that it is not obviously floating in the air.

\subsection{SunCG}

For SunCG dataset, we have quite a lot possible future work to do. In our work, we only generated training and testing images for sofa object in SunCG scenes and now we do only single object detection. In the future, we can first enable multiple objects detection in same category for each image. Then we can generate more categories objects and realize multiple categories object 6D pose prediction. We can also randomize textures of the objects during rendering so that we can have objects in more variety of textures in our training data. Since in real life there is no absolutely same objects as in synthetic scenes, we can randomize per axis scaling to generate objects in different shapes. This idea can be used in data augmentation procedure. In this way, we can generate more realistic scenes with different shapes objects in the same category.

\subsection{ScanNet}

For ScanNet dataset, there are mainly two directions for future work. First of all, increase the size of the training data. Now we are using 264 scenes and 565 scans for our experiments by the time we write this report. ScanNet has in total more than 1500 scans, which means using the whole dataset can enlarge our current training and testing data to 3 times of the current size. With more training data, our model can generalize better to test data and to objects in various shapes of the same category. Also, since we have to filter out objects which have the size of their 3D bounding box lying outside the similarity range with our chosen 3D object model, the more data we have, the smaller range we can set to obtain same number of training data. The smaller the range is, the more similar the objects in our images are, and the better accuracy of the predictions our model can achieve.

Secondly, complete reconstruction is a very import factor to the performance of our model. If any new real dataset are being created, more detailed scans and accurate camera intrinsic parameter records can result in a much better reconstruction, which may make our model predicts much more accurate results for camera localization.